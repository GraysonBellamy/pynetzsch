"""
NETZSCH STA NGB File Parser

This module provides a comprehensive parser for NETZSCH STA (Simultaneous Thermal Analysis)
NGB (NETZSCH Binary) files. These files contain thermal analysis data from NETZSCH instruments
including temperature, mass, heat flow, and other measurement data.

File Format Structure
====================

NGB files are ZIP archives containing multiple binary streams:

- **stream_1.table**: Metadata and instrument configuration
- **stream_2.table**: Primary measurement data (temperature, mass, etc.)
- **stream_3.table**: Additional measurement data (DSC, flow rates, etc.)

Each stream contains binary tables with the following structure:

```
[Table Header][Data Type][Data Payload][End Markers]
```

Binary Markers:
- START_DATA (0xa001): Marks beginning of data payload
- END_DATA: Marks end of data payload
- TABLE_SEPARATOR: Separates individual tables within a stream
- TYPE_PREFIX (0x17fcffff): Precedes data type identifier

Data Types:
- 0x03: 32-bit signed integer (little-endian)
- 0x04: 32-bit IEEE 754 float (little-endian)
- 0x05: 64-bit IEEE 754 double (little-endian)
- 0x1f: UTF-8 string with length prefix

Usage Examples
==============

Basic Usage:
```python
from labetl.netzsch_sta_ngb_parser import load_ngb_data

# Load NGB file and get PyArrow table with embedded metadata
table = load_ngb_data("sample.ngb-ss3")
print(f"Columns: {table.column_names}")
print(f"Rows: {table.num_rows}")

# Access metadata
metadata = table.schema.metadata[b'file_metadata']
```

Advanced Usage with Custom Parser:
```python
from labetl.netzsch_sta_ngb_parser import NGBParser, PatternConfig

# Create parser with custom configuration
config = PatternConfig()
parser = NGBParser(config)

# Parse file and get separate metadata and data
metadata, data_table = parser.parse("sample.ngb-ss3")

# Access specific metadata fields
instrument = metadata.get('instrument', 'Unknown')
sample_mass = metadata.get('sample_mass', 0.0)
```

Error Handling:
```python
from labetl.netzsch_sta_ngb_parser import (
    load_ngb_data,
    NGBParseError,
    NGBCorruptedFileError,
    NGBStreamNotFoundError
)

try:
    table = load_ngb_data("sample.ngb-ss3")
except FileNotFoundError:
    print("File not found")
except NGBCorruptedFileError as e:
    print(f"File is corrupted: {e}")
except NGBStreamNotFoundError as e:
    print(f"Required stream missing: {e}")
except NGBParseError as e:
    print(f"General parsing error: {e}")
```

Performance Considerations
==========================

This parser is optimized for performance using:

1. **NumPy frombuffer()**: For fast binary data conversion
2. **Memory views**: To minimize memory copying
3. **Compiled regex patterns**: Cached for repeated use
4. **Plugin architecture**: Extensible data type handlers

For large files (>100MB), expect parsing times of 1-10 seconds depending on
system performance.

Extending the Parser
====================

Adding New Data Types:
```python
class CustomDataHandler:
    def can_handle(self, data_type: bytes) -> bool:
        return data_type == b"\\x06"  # Custom type

    def parse_data(self, data: bytes) -> List[float]:
        # Custom parsing logic
        return [...]

# Register the handler
parser.binary_parser._data_type_registry.register(CustomDataHandler())
```

Adding New Metadata Fields:
```python
config = PatternConfig()
config.metadata_patterns["custom_field"] = (b"\\x80\\x17", b"\\x50\\x08")
parser = NGBParser(config)
```

Troubleshooting
===============

Common Issues:

1. **"START_DATA marker not found"**:
   - Usually indicates empty or header-only tables
   - Set logging to DEBUG to see which tables are being skipped

2. **"Unknown data type"**:
   - File contains data types not yet supported
   - Implement custom handler or file issue on GitHub

3. **"Shape mismatch when adding column"**:
   - Data columns have different lengths
   - May indicate corrupted data or parsing error

4. **Performance Issues**:
   - Enable DEBUG logging to identify bottlenecks
   - Consider processing files in smaller chunks
   - Ensure sufficient RAM for large files

Logging:
```python
import logging
logging.basicConfig(level=logging.DEBUG)
```

Author: Generated by AI Assistant
License: See project LICENSE file
"""

from __future__ import annotations

import logging
import re
import struct
import zipfile
from dataclasses import dataclass, field
from datetime import datetime, timezone
from enum import Enum
from itertools import tee, zip_longest
from pathlib import Path
from typing import Any, Dict, List, Optional, Protocol, Tuple, TypedDict

import numpy as np
import polars as pl
import pyarrow as pa
import pyarrow.parquet as pq
from polars.exceptions import ShapeError

from pynetzsch.util import get_hash, set_metadata


# -----------------------------------------------------------------------------
# Custom Exceptions
# -----------------------------------------------------------------------------
class NGBParseError(Exception):
    """Base exception for NGB file parsing errors."""

    pass


class NGBCorruptedFileError(NGBParseError):
    """Raised when NGB file is corrupted or has invalid structure."""

    pass


class NGBUnsupportedVersionError(NGBParseError):
    """Raised when NGB file version is not supported."""

    pass


class NGBDataTypeError(NGBParseError):
    """Raised when encountering unknown or invalid data type."""

    pass


class NGBStreamNotFoundError(NGBParseError):
    """Raised when expected stream is not found in NGB file."""

    pass


# -----------------------------------------------------------------------------
# Logging
# -----------------------------------------------------------------------------
logger = logging.getLogger(__name__)
# Avoid "No handler found" warnings for library users.
logger.addHandler(logging.NullHandler())


# -----------------------------------------------------------------------------
# Type Definitions
# -----------------------------------------------------------------------------
class FileMetadata(TypedDict, total=False):
    """Type definition for file metadata dictionary."""

    instrument: str
    project: str
    date_performed: str
    lab: str
    operator: str
    crucible_type: str
    comment: str
    furnace_type: str
    carrier_type: str
    sample_id: str
    sample_name: str
    sample_mass: float
    crucible_mass: float
    material: str
    temperature_program: Dict[str, Dict[str, Any]]
    calibration_constants: Dict[str, float]
    file_hash: Dict[str, str]


class DataTypeHandler(Protocol):
    """Protocol for data type handlers."""

    def can_handle(self, data_type: bytes) -> bool:
        """Check if this handler can process the given data type."""
        ...

    def parse_data(self, data: bytes) -> List[float]:
        """Parse binary data and return list of floats."""
        ...


# -----------------------------------------------------------------------------
# Constants and Configuration
# -----------------------------------------------------------------------------
class DataType(Enum):
    """Binary data type identifiers used in NGB files.

    These constants map to the binary identifiers used in NETZSCH NGB files
    to specify the data type of values stored in the binary format.

    Examples:
        >>> DataType.FLOAT64.value
        b'\\x05'
        >>> data_type == DataType.FLOAT32.value
        True
    """

    INT32 = b"\x03"  # 32-bit signed integer (little-endian)
    FLOAT32 = b"\x04"  # 32-bit IEEE 754 float (little-endian)
    FLOAT64 = b"\x05"  # 64-bit IEEE 754 double (little-endian)
    STRING = b"\x1f"  # UTF-8 string with 4-byte length prefix


@dataclass(frozen=True)
class BinaryMarkers:
    """Binary markers for parsing NGB files.

    These byte sequences mark important boundaries and structures within
    the binary NGB file format. They are used to locate data sections,
    separate tables, and identify data types.

    Attributes:
        END_FIELD: Marks the end of a data field
        TYPE_PREFIX: Precedes data type identifier
        TYPE_SEPARATOR: Separates type from value data
        END_TABLE: Marks the end of a table
        TABLE_SEPARATOR: Separates individual tables in a stream
        START_DATA: Marks beginning of binary data payload
        END_DATA: Marks end of binary data payload

    Example:
        >>> markers = BinaryMarkers()
        >>> data.find(markers.START_DATA)
        1024  # Found at byte position 1024
    """

    END_FIELD: bytes = rb"\x01\x00\x00\x00\x02\x00\x01\x00\x00"
    TYPE_PREFIX: bytes = rb"\x17\xfc\xff\xff"
    TYPE_SEPARATOR: bytes = rb"\x80\x01"
    END_TABLE: bytes = rb"\x18\xfc\xff\xff\x03"
    TABLE_SEPARATOR: bytes = rb"\x00\x00\x01\x00\x00\x00\x0c\x00\x17\xfc\xff\xff\x1a\x80\x01\x01\x80\x02\x00\x00"
    START_DATA: bytes = b"\xa0\x01"
    END_DATA: bytes = (
        b"\x01\x00\x00\x00\x02\x00\x01\x00\x00\x00\x03\x00\x18\xfc\xff\xff\x03\x80\x01"
    )


@dataclass
class PatternConfig:
    """Configuration for metadata and column patterns.

    This class defines the binary patterns used to locate and extract
    specific metadata fields, temperature program data, calibration constants,
    and data columns from NGB files.

    The patterns are defined as tuples of (category_bytes, field_bytes) that
    are used to construct regex patterns for finding specific data fields
    in the binary stream.

    Attributes:
        metadata_patterns: Maps field names to (category, field) byte patterns
        temp_prog_patterns: Patterns for temperature program extraction
        cal_constants_patterns: Patterns for calibration constant extraction
        column_map: Maps hex column IDs to human-readable column names

    Example:
        >>> config = PatternConfig()
        >>> config.column_map["8d"] = "time"
        >>> config.metadata_patterns["sample_id"] = (b"\\x30\\x75", b"\\x98\\x08")

    Note:
        Modifying these patterns may break compatibility with certain
        NGB file versions. Use caution when customizing.
    """

    metadata_patterns: Dict[str, Tuple[bytes, bytes]] = field(
        default_factory=lambda: {
            "instrument": (rb"\x75\x17", rb"\x59\x10"),
            "project": (rb"\x72\x17", rb"\x3c\x08"),
            "date_performed": (rb"\x72\x17", rb"\x3e\x08"),
            "lab": (rb"\x72\x17", rb"\x34\x08"),
            "operator": (rb"\x72\x17", rb"\x35\x08"),
            "crucible_type": (rb"\x7e\x17", rb"\x40\x08"),
            "comment": (rb"\x72\x17", rb"\x3d\x08"),
            "furnace_type": (rb"\x7a\x17", rb"\x40\x08"),
            "carrier_type": (rb"\x79\x17", rb"\x40\x08"),
            "sample_id": (rb"\x30\x75", rb"\x98\x08"),
            "sample_name": (rb"\x30\x75", rb"\x40\x08"),
            "sample_mass": (rb"\x30\x75", rb"\x9e\x0c"),
            "crucible_mass": (rb"\x7e\x17", rb"\x9e\x0c"),
            "material": (rb"\x30\x75", rb"\x62\x09"),
        }
    )
    temp_prog_patterns: Dict[str, bytes] = field(
        default_factory=lambda: {
            "stage_type": rb"\x3f\x08",
            "temperature": rb"\x17\x0e",
            "heating_rate": rb"\x13\x0e",
            "acquisition_rate": rb"\x14\x0e",
            "time": rb"\x15\x0e",
        }
    )
    cal_constants_patterns: Dict[str, bytes] = field(
        default_factory=lambda: {
            f"p{i}": bytes([0x4F + i, 0x04]) if i < 5 else b"\xc3\x04" for i in range(6)
        }
    )
    column_map: Dict[str, str] = field(
        default_factory=lambda: {
            "8d": "time",
            "8e": "temperature",
            "9c": "dsc",
            "9e": "purge_flow",
            "90": "protective_flow",
            "87": "sample_mass",
            "30": "furnace_temperature",
            "32": "furnace_power",
            "33": "h_foil_temp",
            "34": "uc_module",
            "35": "env_pressure",
            "36": "env_accel_x",
            "37": "env_accel_y",
            "38": "env_accel_z",
        }
    )


# -----------------------------------------------------------------------------
# Data Type Handlers and Registry
# -----------------------------------------------------------------------------
class Float64Handler:
    """Handler for 64-bit IEEE 754 double precision floating point data.

    This handler processes binary data containing arrays of 64-bit doubles
    stored in little-endian format. Uses NumPy's frombuffer for optimal
    performance.

    Example:
        >>> handler = Float64Handler()
        >>> handler.can_handle(b'\\x05')  # DataType.FLOAT64.value
        True
        >>> data = b'\\x00\\x00\\x00\\x00\\x00\\x00\\xf0\\x3f'  # 1.0 as double
        >>> handler.parse_data(data)
        [1.0]
    """

    def can_handle(self, data_type: bytes) -> bool:
        return data_type == DataType.FLOAT64.value

    def parse_data(self, data: bytes) -> List[float]:
        arr = np.frombuffer(data, dtype="<f8")
        return [float(x) for x in arr]


class Float32Handler:
    """Handler for 32-bit IEEE 754 single precision floating point data.

    This handler processes binary data containing arrays of 32-bit floats
    stored in little-endian format. Uses NumPy's frombuffer for optimal
    performance.

    Example:
        >>> handler = Float32Handler()
        >>> handler.can_handle(b'\\x04')  # DataType.FLOAT32.value
        True
        >>> data = b'\\x00\\x00\\x80\\x3f'  # 1.0 as float
        >>> handler.parse_data(data)
        [1.0]
    """

    def can_handle(self, data_type: bytes) -> bool:
        return data_type == DataType.FLOAT32.value

    def parse_data(self, data: bytes) -> List[float]:
        arr = np.frombuffer(data, dtype="<f4")
        return [float(x) for x in arr]


class DataTypeRegistry:
    """Registry for data type handlers with pluggable architecture.

    This registry manages a collection of data type handlers that can
    process different binary data formats found in NGB files. New handlers
    can be registered to extend support for additional data types.

    The registry uses a chain-of-responsibility pattern to find the
    appropriate handler for each data type.

    Example:
        >>> registry = DataTypeRegistry()
        >>> registry.parse_data(b'\\x05', binary_data)  # Uses Float64Handler
        [1.0, 2.0, 3.0]

        >>> # Add custom handler
        >>> class CustomHandler:
        ...     def can_handle(self, data_type): return data_type == b'\\x06'
        ...     def parse_data(self, data): return [42.0]
        >>> registry.register(CustomHandler())

    Attributes:
        _handlers: List of registered data type handlers

    Note:
        Handlers are checked in registration order. Register more specific
        handlers before more general ones.
    """

    def __init__(self) -> None:
        self._handlers: List[DataTypeHandler] = []
        self._register_default_handlers()

    def _register_default_handlers(self) -> None:
        """Register default data type handlers."""
        self.register(Float64Handler())
        self.register(Float32Handler())

    def register(self, handler: DataTypeHandler) -> None:
        """Register a new data type handler."""
        self._handlers.append(handler)

    def parse_data(self, data_type: bytes, data: bytes) -> List[float]:
        """Parse data using appropriate handler."""
        for handler in self._handlers:
            if handler.can_handle(data_type):
                return handler.parse_data(data)
        raise NGBDataTypeError(f"No handler found for data type: {data_type.hex()}")


# -----------------------------------------------------------------------------
# Parser Primitives
# -----------------------------------------------------------------------------
class BinaryParser:
    """Handles binary data parsing operations with memory optimization.

    This class provides low-level binary parsing functionality for NGB files,
    including table splitting, data extraction, and value parsing. It uses
    memory-efficient techniques like memoryview to minimize copying.

    The parser maintains compiled regex patterns for performance and includes
    a pluggable data type registry for extensibility.

    Example:
        >>> parser = BinaryParser()
        >>> tables = parser.split_tables(binary_stream_data)
        >>> data = parser.extract_data_array(tables[0], DataType.FLOAT64.value)
        >>> [1.0, 2.0, 3.0, ...]

    Attributes:
        markers: Binary markers used for parsing
        _compiled_patterns: Cache of compiled regex patterns
        _data_type_registry: Registry of data type handlers

    Performance Notes:
        - Uses memoryview to avoid unnecessary memory copies
        - Caches compiled regex patterns for repeated use
        - Leverages NumPy frombuffer for fast array parsing
    """

    def __init__(self, markers: Optional[BinaryMarkers] = None):
        self.markers = markers or BinaryMarkers()
        self._compiled_patterns: Dict[str, re.Pattern[bytes]] = {}
        self._data_type_registry = DataTypeRegistry()
        # Hot-path: table separator compiled once
        self._compiled_patterns["table_sep"] = re.compile(
            self.markers.TABLE_SEPARATOR, re.DOTALL
        )

    def _get_compiled_pattern(self, key: str, pattern: bytes) -> re.Pattern[bytes]:
        """Cache compiled regex patterns for performance."""
        pat = self._compiled_patterns.get(key)
        if pat is None:
            pat = re.compile(pattern, re.DOTALL)
            self._compiled_patterns[key] = pat
        return pat

    @staticmethod
    def parse_value(data_type: bytes, value: bytes) -> Any:
        """Parse binary value based on data type."""
        try:
            if data_type == DataType.INT32.value:
                return struct.unpack("<i", value)[0]
            if data_type == DataType.FLOAT32.value:
                return struct.unpack("<f", value)[0]
            if data_type == DataType.FLOAT64.value:
                return struct.unpack("<d", value)[0]
            if data_type == DataType.STRING.value:
                # Skip 4-byte length; strip nulls.
                return (
                    value[4:]
                    .decode("utf-8", errors="ignore")
                    .strip()
                    .replace("\x00", "")
                )
            return value
        except Exception as e:
            logger.debug("Failed to parse value: %s", e)
            return None

    def split_tables(self, data: bytes) -> List[bytes]:
        """Split binary data into tables using the known separator.

        NGB streams contain multiple tables separated by a specific byte
        sequence. This method efficiently splits the stream into individual
        tables for further processing.

        Args:
            data: Binary data from an NGB stream

        Returns:
            List of binary table data chunks

        Example:
            >>> stream_data = load_stream_from_ngb()
            >>> tables = parser.split_tables(stream_data)
            >>> print(f"Found {len(tables)} tables")
            Found 15 tables

        Note:
            If no separator is found, returns the entire data as a single table.
        """
        pattern = self._compiled_patterns["table_sep"]
        indices = [m.start() - 2 for m in pattern.finditer(data)]
        if not indices:
            return [data]
        start, end = tee(indices)
        next(end, None)
        return [data[i:j] for i, j in zip_longest(start, end)]

    def extract_data_array(self, table: bytes, data_type: bytes) -> List[float]:
        """Extract array of numerical data with memory optimization.

        Extracts arrays of floating-point data from binary tables using
        efficient memory operations and NumPy for fast conversion.

        Args:
            table: Binary table data containing the array
            data_type: Data type identifier (from DataType enum)

        Returns:
            List of floating-point values, empty list if no data found

        Raises:
            NGBDataTypeError: If data type is not supported

        Example:
            >>> table_data = get_table_from_stream()
            >>> values = parser.extract_data_array(table_data, DataType.FLOAT64.value)
            >>> print(f"Extracted {len(values)} data points")
            Extracted 1500 data points

        Performance:
            Uses NumPy frombuffer which is 10-50x faster than struct.iter_unpack
            for large arrays.
        """
        # Use memoryview to avoid unnecessary copying
        table_mv = memoryview(table)

        # Find data boundaries using memoryview
        table_bytes = table_mv.tobytes()  # Only convert once
        start_idx = table_bytes.find(self.markers.START_DATA)
        if start_idx == -1:
            logger.debug("START_DATA marker not found in table")
            return []

        start_idx += 6  # preserve original offset logic
        data_mv = table_mv[start_idx:]

        data_bytes = data_mv.tobytes()  # Only convert once for end search
        end_idx = data_bytes.find(self.markers.END_DATA)
        if end_idx == -1:
            logger.debug("END_DATA marker not found in table")
            return []

        # Extract data chunk efficiently using memoryview slicing
        data_chunk = data_mv[:end_idx].tobytes()

        # Use pluggable data type registry
        try:
            return self._data_type_registry.parse_data(data_type, data_chunk)
        except NGBDataTypeError:
            # Fallback to empty list for unknown data types
            logger.debug(f"Unknown data type: {data_type.hex()}")
            return []


class MetadataExtractor:
    """Extracts metadata from NGB tables with improved type safety."""

    def __init__(self, config: PatternConfig, parser: BinaryParser) -> None:
        self.config = config
        self.parser = parser
        self._compiled_meta: Dict[str, re.Pattern[bytes]] = {}
        self._compiled_temp_prog: Dict[str, re.Pattern[bytes]] = {}
        self._compiled_cal_consts: Dict[str, re.Pattern[bytes]] = {}

        # Precompile patterns used in tight loops for speed (logic unchanged).
        END_FIELD = self.parser.markers.END_FIELD
        TYPE_PREFIX = self.parser.markers.TYPE_PREFIX
        TYPE_SEPARATOR = self.parser.markers.TYPE_SEPARATOR

        for fname, (category, field_bytes) in self.config.metadata_patterns.items():
            pat = (
                category
                + rb".+?"
                + field_bytes
                + rb".+?"
                + TYPE_PREFIX
                + rb"(.+?)"
                + TYPE_SEPARATOR
                + rb"(.+?)"
                + END_FIELD
            )
            self._compiled_meta[fname] = re.compile(pat, re.DOTALL)

        for fname, pat_bytes in self.config.temp_prog_patterns.items():
            pat = (
                pat_bytes
                + rb".+?"
                + TYPE_PREFIX
                + rb"(.+?)"
                + TYPE_SEPARATOR
                + rb"(.+?)"
                + END_FIELD
            )
            self._compiled_temp_prog[fname] = re.compile(pat, re.DOTALL)

        for fname, pat_bytes in self.config.cal_constants_patterns.items():
            pat = (
                pat_bytes
                + rb".+?"
                + TYPE_PREFIX
                + rb"(.+?)"
                + TYPE_SEPARATOR
                + rb"(.+?)"
                + END_FIELD
            )
            self._compiled_cal_consts[fname] = re.compile(pat, re.DOTALL)

    def extract_field(self, table: bytes, field_name: str) -> Optional[Any]:
        """Extract a single metadata field (value only)."""
        if field_name not in self._compiled_meta:
            raise NGBParseError(f"Unknown metadata field: {field_name}")

        pattern = self._compiled_meta[field_name]
        matches = pattern.findall(table)
        if matches:
            data_type, value = matches[0]
            return self.parser.parse_value(data_type, value)
        return None

    def extract_metadata(self, tables: List[bytes]) -> FileMetadata:
        """Extract all metadata from tables with type safety."""
        metadata: FileMetadata = {}

        for table in tables:
            # Standard fields
            for field_name in self._compiled_meta.keys():
                try:
                    value = self.extract_field(table, field_name)
                    if value is not None:
                        if field_name == "date_performed" and isinstance(value, int):
                            value = datetime.fromtimestamp(
                                value, tz=timezone.utc
                            ).isoformat()
                        metadata[field_name] = value  # type: ignore
                except NGBParseError as e:
                    logger.warning(f"Failed to extract field {field_name}: {e}")

            # Temperature program
            self._extract_temperature_program(table, metadata)

            # Calibration constants
            self._extract_calibration_constants(table, metadata)

        return metadata

    def _extract_temperature_program(
        self, table: bytes, metadata: FileMetadata
    ) -> None:
        """Extract temperature program section."""
        CATEGORY = b"\x0c\x2b"
        if CATEGORY not in table:
            return

        step_num = table[0:2].decode("ascii", errors="ignore")[0] if table else "0"
        temp_prog = metadata.setdefault("temperature_program", {})
        step_key = f"step_{step_num}"
        step_data = temp_prog.setdefault(step_key, {})

        for field_name, pattern in self._compiled_temp_prog.items():
            match = pattern.search(table)
            if match:
                data_type, value_bytes = match.groups()
                value = self.parser.parse_value(data_type, value_bytes)
                if value is not None:
                    step_data[field_name] = value

    def _extract_calibration_constants(
        self, table: bytes, metadata: FileMetadata
    ) -> None:
        """Extract calibration constants section."""
        CATEGORY = b"\xf5\x01"
        if CATEGORY not in table:
            return

        cal_constants = metadata.setdefault("calibration_constants", {})
        for field_name, pattern in self._compiled_cal_consts.items():
            match = pattern.search(table)
            if match:
                data_type, value_bytes = match.groups()
                value = self.parser.parse_value(data_type, value_bytes)
                if value is not None:
                    cal_constants[field_name] = value


class DataStreamProcessor:
    """Processes data streams from NGB files with optimized parsing."""

    def __init__(self, config: PatternConfig, parser: BinaryParser) -> None:
        self.config = config
        self.parser = parser
        self._table_sep_re = self.parser._get_compiled_pattern(
            "table_sep", self.parser.markers.TABLE_SEPARATOR
        )

    # --- Stream 2 ---
    def process_stream_2(self, stream_data: bytes) -> pl.DataFrame:
        """Process primary data stream (stream_2)."""
        # Split into tables - exact original logic
        indices = [m.start() - 2 for m in self._table_sep_re.finditer(stream_data)]
        start, end = tee(indices)
        next(end, None)
        stream_table = [stream_data[i:j] for i, j in zip_longest(start, end)]

        output: List[float] = []
        output_polars = pl.DataFrame()
        title: Optional[str] = None

        col_map = self.config.column_map
        markers = self.parser.markers

        for table in stream_table:
            if table[1:2] == b"\x17":  # header
                title = table[0:1].hex()
                title = col_map.get(title, title)
                if len(output) > 1:
                    try:
                        output_polars = output_polars.with_columns(
                            pl.Series(name=title, values=output)
                        )
                    except ShapeError:
                        logger.debug("Shape mismatch when adding column '%s'", title)
                output = []

            if table[1:2] == b"\x75":  # data
                start_data = table.find(markers.START_DATA) + 6
                if start_data == 5:  # find() returned -1
                    logger.debug("START_DATA marker not found in table - skipping")
                    continue

                data = table[start_data:]
                end_data = data.find(markers.END_DATA)
                if end_data == -1:
                    logger.debug("END_DATA marker not found in table - skipping")
                    continue

                data = data[:end_data]
                data_type = table[start_data - 7 : start_data - 6]

                try:
                    parsed_data = self.parser._data_type_registry.parse_data(
                        data_type, data
                    )
                    output.extend(parsed_data)
                except NGBDataTypeError as e:
                    logger.debug(f"Failed to parse data: {e}")
                    continue

        return output_polars

    # --- Stream 3 ---
    def process_stream_3(
        self, stream_data: bytes, existing_df: pl.DataFrame
    ) -> pl.DataFrame:
        """Process secondary data stream (stream_3)."""
        # Split into tables - exact original logic
        indices = [m.start() - 2 for m in self._table_sep_re.finditer(stream_data)]
        start, end = tee(indices)
        next(end, None)
        stream_table = [stream_data[i:j] for i, j in zip_longest(start, end)]

        output: List[float] = []
        output_polars = existing_df
        title: Optional[str] = None

        col_map = self.config.column_map
        markers = self.parser.markers

        for table in stream_table:
            if table[22:25] == b"\x80\x22\x2b":  # header
                title = table[0:1].hex()
                title = col_map.get(title, title)
                output = []

            if table[1:2] == b"\x75":  # data
                start_data = table.find(markers.START_DATA) + 6
                if start_data == 5:  # find() returned -1
                    logger.debug("START_DATA marker not found in table - skipping")
                    continue

                data = table[start_data:]
                end_data = data.find(markers.END_DATA)
                if end_data == -1:
                    logger.debug("END_DATA marker not found in table - skipping")
                    continue

                data = data[:end_data]
                data_type = table[start_data - 7 : start_data - 6]

                try:
                    parsed_data = self.parser._data_type_registry.parse_data(
                        data_type, data
                    )
                    output.extend(parsed_data)
                except NGBDataTypeError as e:
                    logger.debug(f"Failed to parse data: {e}")
                    continue

                # Save after each data block (original behavior)
                try:
                    output_polars = output_polars.with_columns(
                        pl.Series(name=title, values=output)
                    )
                except ShapeError:
                    # Silently ignore shape issues as before
                    pass

        return output_polars


# -----------------------------------------------------------------------------
# Main Parser
# -----------------------------------------------------------------------------
class NGBParser:
    """Main parser for NETZSCH STA NGB files with enhanced error handling.

    This is the primary interface for parsing NETZSCH NGB files. It orchestrates
    the parsing of metadata and measurement data from the various streams within
    an NGB file.

    The parser handles the complete workflow:
    1. Opens and validates the NGB ZIP archive
    2. Extracts metadata from stream_1.table
    3. Processes measurement data from stream_2.table and stream_3.table
    4. Returns structured data with embedded metadata

    Example:
        >>> parser = NGBParser()
        >>> metadata, data_table = parser.parse("sample.ngb-ss3")
        >>> print(f"Sample: {metadata.get('sample_name', 'Unknown')}")
        >>> print(f"Data shape: {data_table.num_rows} x {data_table.num_columns}")
        Sample: Test Sample 1
        Data shape: 2500 x 8

    Advanced Configuration:
        >>> config = PatternConfig()
        >>> config.column_map["custom_id"] = "custom_column"
        >>> parser = NGBParser(config)

    Attributes:
        config: Pattern configuration for parsing
        markers: Binary markers for data identification
        binary_parser: Low-level binary parsing engine
        metadata_extractor: Metadata extraction engine
        data_processor: Data stream processing engine

    Thread Safety:
        This parser is not thread-safe. Create separate instances for
        concurrent parsing operations.
    """

    def __init__(self, config: Optional[PatternConfig] = None) -> None:
        self.config = config or PatternConfig()
        self.markers = BinaryMarkers()
        self.binary_parser = BinaryParser(self.markers)
        self.metadata_extractor = MetadataExtractor(self.config, self.binary_parser)
        self.data_processor = DataStreamProcessor(self.config, self.binary_parser)

    def parse(self, path: str) -> Tuple[FileMetadata, pa.Table]:
        """Parse NGB file and return metadata and Arrow table.

        Opens an NGB file, extracts all metadata and measurement data,
        and returns them as separate objects for flexible use.

        Args:
            path: Path to the .ngb-ss3 file to parse

        Returns:
            Tuple of (metadata_dict, pyarrow_table) where:
            - metadata_dict contains instrument settings, sample info, etc.
            - pyarrow_table contains the measurement data columns

        Raises:
            FileNotFoundError: If the specified file doesn't exist
            NGBStreamNotFoundError: If required streams are missing
            NGBCorruptedFileError: If file structure is invalid
            zipfile.BadZipFile: If file is not a valid ZIP archive

        Example:
            >>> metadata, data = parser.parse("experiment.ngb-ss3")
            >>> print(f"Instrument: {metadata.get('instrument', 'Unknown')}")
            >>> print(f"Columns: {data.column_names}")
            >>> print(f"Temperature range: {data['temperature'].min()} to {data['temperature'].max()}")
            Instrument: NETZSCH STA 449 F3 Jupiter
            Columns: ['time', 'temperature', 'mass', 'dsc', 'purge_flow']
            Temperature range: 25.0 to 800.0

        Performance:
            Typical parsing times:
            - Small files (<1MB): <0.1 seconds
            - Medium files (1-10MB): 0.1-1 seconds
            - Large files (10-100MB): 1-10 seconds
        """
        path_obj = Path(path)
        if not path_obj.exists():
            raise FileNotFoundError(f"File not found: {path}")

        metadata: FileMetadata = {}
        data_df = pl.DataFrame()

        try:
            with zipfile.ZipFile(path, "r") as z:
                # Validate NGB file structure
                available_streams = z.namelist()
                logger.debug(f"Available streams: {available_streams}")

                # stream_1: metadata
                if "Streams/stream_1.table" in available_streams:
                    with z.open("Streams/stream_1.table") as stream:
                        stream_data = stream.read()
                        tables = self.binary_parser.split_tables(stream_data)
                        metadata = self.metadata_extractor.extract_metadata(tables)
                else:
                    raise NGBStreamNotFoundError(
                        "stream_1.table not found - metadata unavailable"
                    )

                # stream_2: primary data
                if "Streams/stream_2.table" in available_streams:
                    with z.open("Streams/stream_2.table") as stream:
                        stream_data = stream.read()
                        data_df = self.data_processor.process_stream_2(stream_data)

                # stream_3: additional data merged into existing df
                if "Streams/stream_3.table" in z.namelist():
                    with z.open("Streams/stream_3.table") as stream:
                        stream_data = stream.read()
                        data_df = self.data_processor.process_stream_3(
                            stream_data, data_df
                        )

        except Exception as e:
            logger.error("Failed to parse NGB file: %s", e)
            raise

        return metadata, data_df.to_arrow()


# -----------------------------------------------------------------------------
# Public API
# -----------------------------------------------------------------------------
def load_ngb_data(path: str) -> pa.Table:
    """
    Load a NETZSCH STA NGB file and return PyArrow table with embedded metadata.

    This is the primary public interface for loading NGB files. It parses the
    file and returns a PyArrow table with all measurement data and metadata
    embedded in the table's schema metadata.

    Parameters
    ----------
    path : str
        The path to the NGB file (.ngb-ss3 or similar extension).
        Supports absolute and relative paths.

    Returns
    -------
    pa.Table
        PyArrow table containing:
        - Measurement data as columns (time, temperature, mass, etc.)
        - Embedded metadata in table.schema.metadata
        - File hash for integrity verification

    Raises
    ------
    FileNotFoundError
        If the specified file does not exist
    NGBStreamNotFoundError
        If required data streams are missing from the NGB file
    NGBCorruptedFileError
        If the file structure is invalid or corrupted
    zipfile.BadZipFile
        If the file is not a valid ZIP archive

    Examples
    --------
    Basic usage:

    >>> import pyarrow as pa
    >>> from labetl.netzsch_sta_ngb_parser import load_ngb_data
    >>>
    >>> # Load NGB file
    >>> table = load_ngb_data("experiment.ngb-ss3")
    >>>
    >>> # Examine structure
    >>> print(f"Shape: {table.num_rows} rows, {table.num_columns} columns")
    >>> print(f"Columns: {table.column_names}")
    Shape: 2500 rows, 8 columns
    Columns: ['time', 'temperature', 'mass', 'dsc', 'purge_flow', ...]

    Accessing metadata:

    >>> # Get embedded metadata
    >>> import json
    >>> metadata_bytes = table.schema.metadata[b'file_metadata']
    >>> metadata = json.loads(metadata_bytes)
    >>>
    >>> print(f"Instrument: {metadata['instrument']}")
    >>> print(f"Sample: {metadata['sample_name']}")
    >>> print(f"Mass: {metadata['sample_mass']} mg")
    Instrument: NETZSCH STA 449 F3 Jupiter
    Sample: Polymer Sample A
    Mass: 15.2 mg

    Working with data:

    >>> # Convert to pandas for analysis
    >>> import polars as pl
    >>> df = pl.from_arrow(table)
    >>>
    >>> # Basic analysis
    >>> temp_range = df['temperature'].min(), df['temperature'].max()
    >>> mass_loss = (df['mass'].first() - df['mass'].last()) / df['mass'].first() * 100
    >>> print(f"Temperature range: {temp_range[0]:.1f} to {temp_range[1]:.1f} °C")
    >>> print(f"Mass loss: {mass_loss:.1f}%")
    Temperature range: 25.0 to 800.0 °C
    Mass loss: 12.3%

    Performance Notes
    -----------------
    - Uses optimized NumPy operations for fast binary parsing
    - Memory-efficient processing with memoryview operations
    - Compiled regex patterns for repeated pattern matching
    - Typical parsing time: 0.1-10 seconds depending on file size

    See Also
    --------
    get_sta_data : Get metadata and data as separate objects
    NGBParser : Low-level parser for advanced use cases
    """
    parser = NGBParser()
    metadata, data = parser.parse(path)

    # Add file hash to metadata
    file_hash = get_hash(path)
    if file_hash is not None:
        metadata["file_hash"] = {
            "file": Path(path).name,
            "method": "BLAKE2b",
            "hash": file_hash,
        }

    # Attach metadata to the Arrow table
    data = set_metadata(data, tbl_meta={"file_metadata": metadata, "type": "STA"})
    return data


def get_sta_data(path: str) -> Tuple[FileMetadata, pa.Table]:
    """
    Get STA data and metadata from an NGB file as separate objects.

    This function provides access to the parsed data and metadata as separate
    objects, which can be useful when you need to work with metadata independently
    of the measurement data.

    Parameters
    ----------
    path : str
        Path to the .ngb-ss3 file to parse.
        Supports absolute and relative paths.

    Returns
    -------
    tuple[FileMetadata, pa.Table]
        A tuple containing:
        - FileMetadata: TypedDict with instrument settings, sample info, etc.
        - pa.Table: PyArrow table with measurement data columns

    Raises
    ------
    FileNotFoundError
        If the specified file does not exist
    NGBStreamNotFoundError
        If required data streams are missing
    NGBCorruptedFileError
        If file structure is invalid

    Examples
    --------
    Basic usage:

    >>> from labetl.netzsch_sta_ngb_parser import get_sta_data
    >>>
    >>> metadata, data = get_sta_data("sample.ngb-ss3")
    >>>
    >>> # Work with metadata
    >>> print(f"Operator: {metadata.get('operator', 'Unknown')}")
    >>> print(f"Date: {metadata.get('date_performed', 'Unknown')}")
    >>>
    >>> # Work with data
    >>> print(f"Data points: {data.num_rows}")
    >>> print(f"Measurements: {data.column_names}")
    Operator: John Doe
    Date: 2024-03-15T10:30:00+00:00
    Data points: 2500
    Measurements: ['time', 'temperature', 'mass', 'dsc']

    Advanced metadata access:

    >>> # Access temperature program
    >>> temp_program = metadata.get('temperature_program', {})
    >>> for step, params in temp_program.items():
    ...     print(f"{step}: {params}")
    step_1: {'heating_rate': 10.0, 'temperature': 800.0, 'time': 80.0}

    >>> # Access calibration constants
    >>> cal_constants = metadata.get('calibration_constants', {})
    >>> print(f"Calibration: {cal_constants}")
    Calibration: {'p0': 1.0, 'p1': 0.98, 'p2': 0.001}

    Data analysis workflow:

    >>> import polars as pl
    >>>
    >>> # Convert to DataFrame
    >>> df = pl.from_arrow(data)
    >>>
    >>> # Filter data using metadata
    >>> initial_mass = metadata.get('sample_mass', 0)
    >>> if initial_mass > 0:
    ...     df = df.with_columns([
    ...         (pl.col('mass') / initial_mass * 100).alias('mass_percent')
    ...     ])
    >>>
    >>> print(df.head())

    See Also
    --------
    load_ngb_data : Load data with embedded metadata in PyArrow table
    NGBParser : Low-level parser class for custom processing
    """
    parser = NGBParser()
    return parser.parse(path)


# -----------------------------------------------------------------------------
# Extended functionality for future development
# -----------------------------------------------------------------------------
class NGBParserExtended(NGBParser):
    """Extended parser with additional capabilities."""

    def __init__(
        self, config: Optional[PatternConfig] = None, cache_patterns: bool = True
    ):
        super().__init__(config)
        self.cache_patterns = cache_patterns
        self._pattern_cache: Dict[str, re.Pattern] = {}

    def add_custom_column_mapping(self, hex_id: str, column_name: str) -> None:
        """Add custom column mapping at runtime."""
        self.config.column_map[hex_id] = column_name

    def add_metadata_pattern(
        self, field_name: str, category: bytes, field: bytes
    ) -> None:
        """Add custom metadata pattern at runtime."""
        self.config.metadata_patterns[field_name] = (category, field)

    def parse_with_validation(self, path: str) -> Tuple[FileMetadata, pa.Table]:
        """Parse with additional validation."""
        metadata, data = self.parse(path)

        # Validate required columns
        required_columns = ["time", "temperature"]
        schema = data.schema
        missing = [col for col in required_columns if col not in schema.names]
        if missing:
            logger.warning("Missing required columns: %s", missing)

        # Validate data ranges
        if "temperature" in schema.names:
            temp_col = data.column("temperature").to_pylist()
            if temp_col and (min(temp_col) < -273.15 or max(temp_col) > 3000):
                logger.warning("Temperature values outside expected range")

        return metadata, data


# -----------------------------------------------------------------------------
# CLI
# -----------------------------------------------------------------------------
def main() -> int:
    """Command-line interface for the NGB parser.

    Provides a command-line tool for parsing NGB files and converting
    them to various output formats including Parquet and CSV.

    Usage:
        python -m labetl.netzsch_sta_ngb_parser input.ngb-ss3 [options]

    Examples:
        # Parse to Parquet (default)
        python -m labetl.netzsch_sta_ngb_parser sample.ngb-ss3

        # Parse to CSV with verbose logging
        python -m labetl.netzsch_sta_ngb_parser sample.ngb-ss3 -f csv -v

        # Parse to both formats in custom directory
        python -m labetl.netzsch_sta_ngb_parser sample.ngb-ss3 -f all -o /output/dir

    Returns:
        int: Exit code (0 for success, 1 for error)
    """
    import argparse

    parser_cli = argparse.ArgumentParser(description="Parse NETZSCH STA NGB files")
    parser_cli.add_argument("input", help="Input NGB file path")
    parser_cli.add_argument("-o", "--output", help="Output directory", default=".")
    parser_cli.add_argument(
        "-f",
        "--format",
        choices=["parquet", "csv", "all"],
        default="parquet",
        help="Output format",
    )
    parser_cli.add_argument(
        "-v", "--verbose", action="store_true", help="Enable verbose logging"
    )

    args = parser_cli.parse_args()

    logging.basicConfig(level=(logging.DEBUG if args.verbose else logging.INFO))

    try:
        data = load_ngb_data(args.input)
        output_path = Path(args.output)
        output_path.mkdir(parents=True, exist_ok=True)

        base_name = Path(args.input).stem
        if args.format in ("parquet", "all"):
            pq.write_table(
                data, output_path / f"{base_name}.parquet", compression="snappy"
            )
        if args.format in ("csv", "all"):
            df = pl.from_arrow(data).to_pandas()
            df.to_csv(output_path / f"{base_name}.csv", index=False)

        logger.info("Successfully parsed %s", args.input)
        return 0
    except Exception as e:
        logger.error("Failed to parse file: %s", e)
        return 1


if __name__ == "__main__":  # pragma: no cover
    raise SystemExit(main())
